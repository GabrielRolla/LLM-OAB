{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações Gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-23T23:09:32.629051Z",
     "iopub.status.busy": "2025-04-23T23:09:32.628568Z",
     "iopub.status.idle": "2025-04-23T23:09:33.875268Z",
     "shell.execute_reply": "2025-04-23T23:09:33.874085Z",
     "shell.execute_reply.started": "2025-04-23T23:09:32.629017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações para modelos \"locais\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T23:09:35.302605Z",
     "iopub.status.busy": "2025-04-23T23:09:35.302098Z",
     "iopub.status.idle": "2025-04-23T23:09:39.623121Z",
     "shell.execute_reply": "2025-04-23T23:09:39.621821Z",
     "shell.execute_reply.started": "2025-04-23T23:09:35.302568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=jsCUDeg_Op4&t=185s\n",
    "\n",
    "!pip install -q -U transformers accelerate bitsandbytes\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, pipeline\n",
    "import shutil\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import transformers\n",
    "from transformers import BitsAndBytesConfig\n",
    "from accelerate import dispatch_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gateways\n",
    "Os gateways são responsáveis por chamar as APIs dos LLMs. Os gateways devem receber todos os parametros que serão passados para a API ja tratados. Além disso, os gateways devem tratar as respostas para que seja retornado apenas o texto de resposta já tratado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T23:09:47.089376Z",
     "iopub.status.busy": "2025-04-23T23:09:47.088387Z",
     "iopub.status.idle": "2025-04-23T23:09:47.101538Z",
     "shell.execute_reply": "2025-04-23T23:09:47.100437Z",
     "shell.execute_reply.started": "2025-04-23T23:09:47.089338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class LLMGateway(ABC):\n",
    "    def __init__(self, model_name: str, prompt_template: str):\n",
    "        self.model_name = model_name\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    @abstractmethod\n",
    "    def sendRequestToAPI(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Método abstrato que envia a requisição para o LLM e retorna a resposta.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class GeminiGateway(LLMGateway):\n",
    "    def __init__(self, model_name: str, prompt_template: str, api_key: str):\n",
    "        super().__init__(model_name, prompt_template)\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def sendRequestToAPI(self, prompt: str) -> str:\n",
    "        generation_config = {\n",
    "            \"candidate_count\": 1,\n",
    "        }\n",
    "        safety_settings = {\n",
    "            \"HARASSMENT\": \"BLOCK_NONE\",\n",
    "            \"HATE\": \"BLOCK_NONE\",\n",
    "            \"SEXUAL\": \"BLOCK_NONE\",\n",
    "            \"DANGEROUS\": \"BLOCK_NONE\",\n",
    "        }\n",
    "\n",
    "        nome_modelo = self.model_name.replace(\"google/\", \"models/\")\n",
    "        genai.configure(api_key=self.api_key)\n",
    "\n",
    "        model = genai.GenerativeModel(\n",
    "            model_name=nome_modelo,\n",
    "            generation_config=generation_config,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            resposta_completa = model.generate_content(prompt)\n",
    "            resposta = resposta_completa.text\n",
    "        except Exception as e:\n",
    "            print(\"Erro inesperado:\", e)\n",
    "            return \"\"\n",
    "\n",
    "        time.sleep(30)  # Pausa para respeitar limites da API, se necessário\n",
    "        return resposta\n",
    "\n",
    "\n",
    "class HuggingFaceGateway(LLMGateway):\n",
    "    def __init__(self, model_name: str, prompt_template: str, quantization_config):\n",
    "        super().__init__(model_name, prompt_template)\n",
    "        self.quantization_config = quantization_config\n",
    "\n",
    "    def sendRequestToAPI(self) -> str: ##load do modelo\n",
    "        \n",
    "        if self.model_name == \"google/flan-t5-small\":\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config= self.quantization_config,\n",
    "                device_map=\"auto\",  # Automatically map the model to available devices (cpu or gpu)\n",
    "            )\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config= self.quantization_config,\n",
    "                device_map=\"cuda\",  # Automatically map the model to available gpu\n",
    "            )\n",
    "    \n",
    "        # Create the pipeline\n",
    "        pipe = pipeline(\n",
    "           \"text2text-generation\" if model == \"google/flan-t5-small\" else \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        return pipe, model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conectores\n",
    "Os conectores são responsáveis por receber os parâmetros para a realização dos testes e trata-los para que possam ser passados para a api do LLM. Nos conectores são definidas configurações padrão como chaves de API, temperatura, quantização dos modelos, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T23:09:49.387586Z",
     "iopub.status.busy": "2025-04-23T23:09:49.387203Z",
     "iopub.status.idle": "2025-04-23T23:09:49.399832Z",
     "shell.execute_reply": "2025-04-23T23:09:49.398762Z",
     "shell.execute_reply.started": "2025-04-23T23:09:49.387553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LLMConnector(ABC):\n",
    "    def __init__(self, model_name: str, prompt_template: str):\n",
    "        self.model_name = model_name\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    @abstractmethod\n",
    "    def callGateway(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Método abstrato que envia a requisição para o LLM e retorna a resposta.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class GeminiConnector(LLMConnector):\n",
    "    def __init__(self, model_name: str, prompt_template: str):\n",
    "         super().__init__(model_name, prompt_template)\n",
    "         self.api_key = '[GEMINI_API_KEY]'\n",
    "    \n",
    "    def callGateway(self):\n",
    "        gateway = GeminiGateway(model_name=\"google/gemini-2.0-flash\", prompt_template=\"\", api_key= self.api_key); \n",
    "        response = gateway.sendRequestToAPI(self.prompt_template); \n",
    "        return response;\n",
    "\n",
    "class HuggingFaceConnector(LLMConnector):\n",
    "    def __init__(self, model_name: str, prompt_template: str):\n",
    "        super().__init__(model_name, prompt_template)\n",
    "        self.api_key = '[HUGGING_FACE_TOKEN]'\n",
    "        self.quantization = False\n",
    "        self.pipe = None\n",
    "        self.model = None\n",
    "\n",
    "        \n",
    "        from huggingface_hub import login\n",
    "        login(token=self.api_key)\n",
    "        print('Loged on hugging face')\n",
    "\n",
    "    def callGateway(self):\n",
    "        if self.pipe is not None:\n",
    "            response = self.pipe(self.prompt_template, max_length=999, num_return_sequences=1, truncation=True)\n",
    "            return response[0]['generated_text'].replace(self.prompt_template,\"\").strip()\n",
    "        # Enable quantization using bitsandbytes (e.g., 4-bit)\n",
    "        if self.quantization:\n",
    "          quantization_config = BitsAndBytesConfig(\n",
    "              load_in_4bit=True,  # Enable 4-bit quantization\n",
    "              bnb_4bit_compute_dtype=torch.float16,  # Use float16 for computations\n",
    "              bnb_4bit_use_double_quant=True,  # Use double quantization for memory efficiency\n",
    "              bnb_4bit_quant_type=\"nf4\"  # Use normalized float4 for better accuracy\n",
    "          )\n",
    "        else:\n",
    "          quantization_config = BitsAndBytesConfig(\n",
    "              load_in_4bit=False,  # Enable 4-bit quantization\n",
    "              bnb_4bit_compute_dtype=torch.float32,  # Use float16 for computations\n",
    "              bnb_4bit_use_double_quant=False,  # Use double quantization for memory efficiency\n",
    "              bnb_4bit_quant_type=\"nf4\"  # Use normalized float4 for better accuracy\n",
    "          )\n",
    "        gateway = HuggingFaceGateway(self.model_name, self.prompt_template, quantization_config)\n",
    "        self.pipe, self.model = gateway.sendRequestToAPI()\n",
    "        response = self.pipe(self.prompt_template, max_length=999, num_return_sequences=1, truncation=True)\n",
    "        return response[0]['generated_text'].replace(self.prompt_template,\"\").strip()\n",
    "        # self.unloadModel(model, pipe, prompt_template, quantization_config)\n",
    "\n",
    "    # def unloadModel(model, pipe):\n",
    "    #     return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolvedor de provas\n",
    "Essa  é a classe que resolve as provas da OAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T23:09:53.133357Z",
     "iopub.status.busy": "2025-04-23T23:09:53.132234Z",
     "iopub.status.idle": "2025-04-23T23:09:53.150338Z",
     "shell.execute_reply": "2025-04-23T23:09:53.149070Z",
     "shell.execute_reply.started": "2025-04-23T23:09:53.133284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TestMaker:\n",
    "    def __init__(self, model_name: str, prompt_template: str, oabExamDf: pd.DataFrame, hint: bool):\n",
    "        self.model_name = model_name\n",
    "        self.prompt_template = prompt_template\n",
    "        self.oabExamDf = oabExamDf  # agora é um DataFrame\n",
    "        self.hint = hint\n",
    "        self.huggingFaceConnector = None  # <- aqui armazenamos o conector\n",
    "\n",
    "\n",
    "    def callLLMConnector(self, prompt):\n",
    "        if(self.model_name == 'google/gemini-2.0-flash'):\n",
    "            connector = GeminiConnector(\n",
    "            model_name= self.model_name,\n",
    "            prompt_template=prompt,\n",
    "            )\n",
    "\n",
    "            return connector.callGateway()\n",
    "\n",
    "        else:\n",
    "            if(self.huggingFaceConnector is None):\n",
    "                self.huggingFaceConnector = HuggingFaceConnector(\n",
    "                model_name=self.model_name,\n",
    "                prompt_template=prompt  \n",
    "            )\n",
    "                \n",
    "            self.huggingFaceConnector.prompt_template = prompt\n",
    "            return self.huggingFaceConnector.callGateway()\n",
    "\n",
    "    def buildPrompt(self, question: str, legalTools: str) -> str:\n",
    "        prompt = self.prompt_template.format(\n",
    "            question = question,\n",
    "            legalTools = f\"Utilize exclusivamente o ferramental juridico fornecido para responder a questão. Não útilize outras leis além das fornecidas:  {legalTools}\" if self.hint else \"\"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def getChosenAlternative(self, response):\n",
    "        match = re.search(r\"Alternativa:\\s*\\*{0,2}([A-Ea-e])\\*{0,2}\", response, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "        else: \n",
    "            print('Erro ao separar alternativa');\n",
    "        \n",
    "\n",
    "    def makeTest(self):\n",
    "        connectorParams = {}\n",
    "        resolvedTest = pd.DataFrame()\n",
    "        llmAnswerList = []\n",
    "        \n",
    "        for index, question in enumerate(self.oabExamDf['question'], start=0):\n",
    "            print(f'Resolvendo questão {index + 1}')\n",
    "            legalTools = self.oabExamDf['legalPrinciples'].iloc[index]\n",
    "            prompt = self.buildPrompt(question,legalTools)\n",
    "            start_time = time.time()\n",
    "            llmResponse = self.callLLMConnector(prompt)\n",
    "            end_time = time.time()\n",
    "            alternative = self.getChosenAlternative(llmResponse)\n",
    "\n",
    "            row = pd.DataFrame([{\n",
    "                'full_model_name': self.model_name,\n",
    "                'model_family': self.model_name.split('/')[0],\n",
    "                'llm_chosen_answer': alternative,\n",
    "                'is_correct': True if alternative == self.oabExamDf['answers'].iloc[index] else False,\n",
    "                'prompt': prompt,\n",
    "                'llm_response': llmResponse,\n",
    "                'token_size_question': len(question.split()),\n",
    "                'token_size_response':len(llmResponse.split()),\n",
    "                'elapsed_time_sec': end_time - start_time,\n",
    "                'hint': self.hint\n",
    "            }])\n",
    "            \n",
    "            resolvedTest = pd.concat([resolvedTest, row], ignore_index=True)\n",
    "\n",
    "        return resolvedTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerar csv de provas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T23:09:58.460222Z",
     "iopub.status.busy": "2025-04-23T23:09:58.459844Z",
     "iopub.status.idle": "2025-04-23T23:09:58.465240Z",
     "shell.execute_reply": "2025-04-23T23:09:58.464236Z",
     "shell.execute_reply.started": "2025-04-23T23:09:58.460201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\" Responda a questão da OAB abaixo apresentando a alternativa correta e a justificativa da escolha. \n",
    "-Questão:\n",
    "{question}\n",
    "\n",
    "{legalTools}\n",
    "\n",
    "Retorne a resposta **EXCLUSIVAMENTE** no seguinte formato:\n",
    "'Alternativa: LETRA, Justificativa: Explicação da escolha'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T23:10:00.341904Z",
     "iopub.status.busy": "2025-04-23T23:10:00.341480Z",
     "iopub.status.idle": "2025-04-23T23:51:43.838044Z",
     "shell.execute_reply": "2025-04-23T23:51:43.836736Z",
     "shell.execute_reply.started": "2025-04-23T23:10:00.341875Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolvendo questão 1\n",
      "Resolvendo questão 2\n",
      "Resolvendo questão 3\n",
      "Resolvendo questão 4\n",
      "Resolvendo questão 5\n",
      "Resolvendo questão 6\n",
      "Resolvendo questão 7\n",
      "Resolvendo questão 8\n",
      "Resolvendo questão 9\n",
      "Resolvendo questão 10\n",
      "Resolvendo questão 11\n",
      "Resolvendo questão 12\n",
      "Resolvendo questão 13\n",
      "Resolvendo questão 14\n",
      "Resolvendo questão 15\n",
      "Resolvendo questão 16\n",
      "Resolvendo questão 17\n",
      "Resolvendo questão 18\n",
      "Resolvendo questão 19\n",
      "Resolvendo questão 20\n",
      "Resolvendo questão 21\n",
      "Resolvendo questão 22\n",
      "Resolvendo questão 23\n",
      "Resolvendo questão 24\n",
      "Resolvendo questão 25\n",
      "Resolvendo questão 26\n",
      "Resolvendo questão 27\n",
      "Resolvendo questão 28\n",
      "Resolvendo questão 29\n",
      "Resolvendo questão 30\n",
      "Resolvendo questão 31\n",
      "Resolvendo questão 32\n",
      "Resolvendo questão 33\n",
      "Resolvendo questão 34\n",
      "Resolvendo questão 35\n",
      "Resolvendo questão 36\n",
      "Resolvendo questão 37\n",
      "Resolvendo questão 38\n",
      "Resolvendo questão 39\n",
      "Resolvendo questão 40\n",
      "Resolvendo questão 41\n",
      "Resolvendo questão 42\n",
      "Resolvendo questão 43\n",
      "Resolvendo questão 44\n",
      "Resolvendo questão 45\n",
      "Resolvendo questão 46\n",
      "Resolvendo questão 47\n",
      "Resolvendo questão 48\n",
      "Resolvendo questão 49\n",
      "Resolvendo questão 50\n",
      "Resolvendo questão 51\n",
      "Resolvendo questão 52\n",
      "Resolvendo questão 53\n",
      "Resolvendo questão 54\n",
      "Resolvendo questão 55\n",
      "Resolvendo questão 56\n",
      "Resolvendo questão 57\n",
      "Resolvendo questão 58\n",
      "Resolvendo questão 59\n",
      "Resolvendo questão 60\n",
      "Resolvendo questão 61\n",
      "Resolvendo questão 62\n",
      "Resolvendo questão 63\n",
      "Resolvendo questão 64\n",
      "Resolvendo questão 65\n",
      "Resolvendo questão 66\n",
      "Resolvendo questão 69\n",
      "Resolvendo questão 70\n",
      "Resolvendo questão 71\n",
      "Resolvendo questão 72\n",
      "Resolvendo questão 73\n",
      "Resolvendo questão 74\n",
      "Resolvendo questão 75\n",
      "Resolvendo questão 76\n",
      "Resolvendo questão 77\n",
      "Resolvendo questão 78\n",
      "Resolvendo questão 79\n",
      "Resolvendo questão 80\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'google/gemini-2.0-flash'\n",
    "HINT = False\n",
    "FILE_PATH = '/kaggle/input/oab-oficial-datasets/OAB-38.csv'\n",
    "\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# Instanciando o testMaker\n",
    "tm = TestMaker(\n",
    "    model_name= MODEL,\n",
    "    prompt_template= PROMPT_TEMPLATE,\n",
    "    oabExamDf=df,\n",
    "    hint= HINT,\n",
    ")\n",
    "\n",
    "# Executando o teste\n",
    "response_df = tm.makeTest()\n",
    "df_final = pd.concat([df.reset_index(drop=True), response_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "oabVersion = FILE_PATH.split('/')[4].split('.')[0].lower()\n",
    "dfName = MODEL.replace('/', '_') + '_' + oabVersion + '_' + 'portuguese' + '_' + ('hint' if HINT else 'noHint')\n",
    "\n",
    "df_final.to_csv(f\"/kaggle/working/{dfName.lower()}.csv\", index=False)\n",
    "df_final.to_excel(f\"/kaggle/working/{dfName.lower()}.xlsx\", index=False, engine=\"openpyxl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T22:40:16.284633Z",
     "iopub.status.busy": "2025-04-23T22:40:16.283613Z",
     "iopub.status.idle": "2025-04-23T22:40:16.295277Z",
     "shell.execute_reply": "2025-04-23T22:40:16.294292Z",
     "shell.execute_reply.started": "2025-04-23T22:40:16.284603Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>comment</th>\n",
       "      <th>legalPrinciples</th>\n",
       "      <th>answers</th>\n",
       "      <th>canceledQuestion?</th>\n",
       "      <th>full_model_name</th>\n",
       "      <th>model_family</th>\n",
       "      <th>llm_chosen_answer</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>prompt</th>\n",
       "      <th>llm_response</th>\n",
       "      <th>token_size_question</th>\n",
       "      <th>token_size_response</th>\n",
       "      <th>elapsed_time_sec</th>\n",
       "      <th>hint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [question, comment, legalPrinciples, answers, canceledQuestion?, full_model_name, model_family, llm_chosen_answer, is_correct, prompt, llm_response, token_size_question, token_size_response, elapsed_time_sec, hint]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['llm_chosen_answer'].isna()]\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7026616,
     "sourceId": 11245745,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7193870,
     "sourceId": 11477997,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7194315,
     "sourceId": 11478660,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
